{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de résumé\n",
    "\n",
    "L'objectif de ce notebook est de comparer les performances de génération de résumé entre un modèle baseline et le modèle T5 (Text-to-Text Transfer Transformer de Google Research). \n",
    "\n",
    "### Dataset\n",
    "- Présentation de CNN /Daily Mail\n",
    "- Exploration aléatoire du dataset\n",
    "### Modèle baseline: TextRank\n",
    "- Génération des résumés\n",
    "- Score Rouge\n",
    "### Modèle T5\n",
    "- Préparation des données\n",
    "- Prétraitement des données\n",
    "- Fine-tuning du modèle\n",
    "- Génération de résumés et calcul du score Rouge\n",
    "- Optimisation du modèle: changement du nombre d'epochs\n",
    "- Sauvegarde des articles et résumés générés dans un fichier CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Présentation de CNN/Daily Mail\n",
    "On utilise le dataset CNN/Daily Mail fourni par Hugging Face.  \n",
    "Le CNN/Daily Mail dataset est largement utilisé dans la communauté de recherche en NLP pour évaluer les performances des algorithmes de résumé automatique.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque entrée dans le dataset contient deux parties principales :  \n",
    "Article : l'article complet provenant de CNN ou de Daily Mail. Mélange d'informations factuelles, de descriptions et de commentaires.  \n",
    "Highlights: les résumés \"Highlights\" fournissent une référence de qualité pour évaluer les résumés générés par des modèles automatiques.  \n",
    "Pour éviter des calculs trop longs, on se limitera à environ 130 lignes du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0', split='validation[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 29.57ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "491882"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path = 'cnn_dailymail_origine.csv'\n",
    "dataset.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration aléatoire du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': '(CNN)A U.S. Air Force veteran who allegedly tried to join ISIS in Syria but was turned back by Turkish authorities before he could get to the war-torn country entered a not guilty plea to terror-related charges Wednesday in a federal court in New York. Tairod Nathan Webster Pugh, accused of making the foiled attempt in January, was indicted by a grand jury on charges of trying to give material support to the terror group and obstruction of justice, the U.S. Justice Department said in a two-count indictment announced Tuesday. Among the evidence, prosecutors allege: Investigators discovered on his laptop computer a letter saying he wanted to \"use the talents and skills given to me by Allah to establish and defend the Islamic States,\" and a chart of crossing points between Turkey and Syria, where ISIS controls some territory. Who has been recruited to ISIS from the West? Pugh, a 47-year-old convert to Islam and a former New Jersey resident who served in the Air Force from 1986 to 1990, was arrested upon his return to the United States in January, the Justice Department said. \"Pugh, an American citizen and former member of our military, allegedly abandoned his allegiance to the United States and sought to provide material support to ISIL,\" Assistant U.S. Attorney General John Carlin said, using an alternate acronym for the Islamist terror group that controls territory in parts of Iraq and Syria. At his arraignment Wednesday in U.S. District Court in Brooklyn, Pugh appeared in a black T-shirt and khaki pants and stated his name. He pleaded not guilty through his attorney, Michael Schneider. The defendant, a former avionics instrument system specialist in the Air Force, flew from Egypt to Turkey on January 10, weeks after being fired from a Middle East-based job as an airplane mechanic, U.S. authorities allege. Why is ISIS so successful at luring Westerners? But Turkey denied him entry. In the indictment, U.S. authorities said Turkey was likely suspicious Pugh was headed for Syria. Instead Turkish officials sent him on a return flight to Egypt, where he was detained. In Egypt, he was carrying multiple electronic devices, \"including four USB thumb drives that had been stripped of their plastic casings and an iPod that had been wiped clean of data,\" the Justice Department said in a statement. Pugh had purposefully tampered with the devices to prevent others from getting access to his electronic media, the indictment said. Pugh was deported to the United States, where agents with the FBI\\'s Joint Terrorism Task Force obtained a warrant for his devices, including the laptop, the department said. Investigators found a letter from January addressed to a Misha, whom they believe is his wife, authorities said. In it, the writer says: \"I am a (mujahedeen). I am a sword against the oppressor and a shield for the oppressed. I will use the talents and skills given to me by Allah to establish and defend the Islamic States.\" In addition to that letter and the Turkey/Syria border chart, agents also found recent Internet searches for information on \"borders controlled by Islamic state,\" as well as \"Internet searches for \\'Flames of War,\\' an ISIL propaganda video,\" and \"downloaded videos, including one showing ISIL members executing prisoners,\" the Justice Department said. They also found what the government said was another 180 jihadist propaganda videos. Pugh was arrested in Asbury Park, New Jersey, on January 16. While in the Air Force, Pugh was trained in installing and maintaining aircraft engine, navigation and weapons systems, the Justice Department said. Pugh converted to Islam after moving to San Antonio in 1998, according to the indictment. The indictment said he took a job as a mechanic with American Airlines in or about 2001. The airline has not responded to a request for comment. In 2001, an American Airlines co-worker alerted the FBI that Pugh \"sympathized with Osama bin Laden, felt that the 1998 bombings of U.S. embassies overseas were justified, and expressed anti-American sentiment,\" the complaint said. One year later, an associate told the FBI that Pugh had expressed interest in traveling to Chechnya to \"fight jihad.\" From October 2009 to March 2010, he worked in Iraq as an Army contractor for DynCorp, according to the complaint. According to Pugh\\'s LinkedIn page, he listed himself as a maintenance manager for Gryphon Airlines, a Kuwait-based charter airline, since September 2014. But the airline told CNN that Pugh was only under consideration to work for it in 2014. \"In third quarter 2014, Mr. Pugh was under consideration for a future Gryphon project, but did not meet the qualifications,\" the airline said in a statement. \"Gryphon declined to hire Mr. Pugh. Gryphon personnel are cooperating with the authorities.\" His last known U.S. address was in Neptune, New Jersey, but he had lived in Egypt for about the last year, the indictment said. If convicted, Pugh could be sentenced to up to 35 years in prison. CNN\\'s Ray Sanchez, John Newsome and Steve Almasy contributed to this report.', 'highlights': 'Tairod Nathan Webster Pugh enters not guilty plea to terror-related charges .\\nPugh flew to Turkey on January 10, authorities say .\\nAuthorities allege a letter on his laptop told his wife he was a mujahedeen .', 'id': '66af1b070749683fb7a18ddc76e1fbb1439882db'}\n",
      "{'article': 'Boston (CNN)Eight minutes of sheer terror. That\\'s what police officers from Watertown, Massachusetts, described in heart-stopping detail Monday, revealing details of a chaotic shootout with the Boston Marathon bombing suspects. One of the suspects, Tamerlan Tsarnaev, died from injuries he sustained that day -- wounded in a gun battle, then run over by his brother, Dzhokhar. Dzhokhar Tsarnaev managed to escape, but was later caught by police. Now he\\'s on trial, facing 30 charges for the marathon bombings, which killed three people and injured more than 260 others. The shootout with police in the early morning hours of April 19, 2013, marked one of the most dramatic chapters in the manhunt for the suspects who paralyzed the Boston metropolitan area for days. It started, patrolman Joseph Reynolds testified, soon after he locked eyes with Tamerlan Tsarnaev, who he spotted driving an SUV that matched a description of a stolen vehicle. Reynolds called for backup. It wasn\\'t long before Tamerlan and Dzhokhar Tsarnaev were firing guns, throwing bombs and trying to run officers over with a stolen SUV, Reynolds said. \"Tamerlan Tsarnaev got out of the driver\\'s side door and began shooting at my cruiser,\" Reynolds said. Soon, the officer said he ran out of bullets. Sgt. Jeffrey Pugliese saw what looked like muzzle flashes as soon as he arrived at the scene. \"I put my vehicle in park, I took a round through the windshield, I was sprayed with glass and I knew, OK, we were being fired on,\" said Sgt. John MacLellan. Then the two brothers began throwing improvised explosives, including pipe bombs and a pressure cooker bomb, the officers testified. \"I noticed one was bigger than the other, and they had different styles when they were throwing the devices,\" MacLellan said. \"One was throwing like a baseball.\" MacLellan said the pressure cooker bomb \"was incredibly loud. I had to holster my weapon. My eyes were shaking violently in my head. I couldn\\'t see.\" Pugliese said he opened fire when he saw one of the men, Tamerlan Tsarnaev, charging toward him. Pugliese fired and the man threw his pistol at the officer, hitting him in the bicep. Pugliese tackled him. And with Tamerlan Tsarnaev, who was wounded from gunfire, on the ground, the three officers tried to put him in handcuffs. They thought they\\'d be able to arrest him. But then something changed. \"We were wrestling with Tamerlan, and all of a sudden I could hear an engine revving,\" Reynolds said . The SUV, Reynolds said, was heading straight toward the officers. The dramatic descriptions of the shootout Monday sounded like a scene from a Hollywood film, said Boston Globe columnist Kevin Cullen. But the most extraordinary revelation in court, he said, was that the night could have ended very differently. \"Dzhokhar Tsarnaev did not have to go back and run his brother over. He actually did a three-point turn and reversed the vehicle. He could have sped off and run away,\" Cullen told CNN\\'s \"The Lead with Jake Tapper.\" He was in a much better position to flee. But apparently he decided to do a U-turn and come back.\" The sight of the stolen SUV speeding toward them caught the officers by surprise. \"I reached down and I grabbed Tamerlan by the back of the belt and tried to drag him out of the street so he wouldn\\'t be hit,\" Pugliese said. \"The black SUV, it was right in my face. ... I kind of laid back and felt the wind from the vehicle as it went by.\" But they didn\\'t move Tamerlan in time. His body became hung up in the rear wheels and he was dragged a short distance, Pugliese said. The prosecutor asked Pugliese if there was something in the road that forced the SUV driver to go directly at the officers. \"No,\" he said. \"It was accelerating at a very high rate of speed.\" Later that day, Tamerlan Tsarnaev was pronounced dead at a local hospital, with the cause listed as \"traumatic injuries\" to the head and torso. His fingerprints led to the identification of the suspects. Officers discovered that Richard Donohue, a Massachusetts Bay Transportation Authority officer, had been hit by friendly fire during the shootout. He survived, but nearly bled to death. Dzhokhar Tsarnaev was arrested about 8:45 p.m. that same day, hiding in a boat called the \"Slip Away\" that was stowed in a backyard in Watertown. Jurors saw photos of the boat last week. But on Monday, they got a chance to see the boat, which has become a key piece of evidence in the trial, in person. Before the trial started, prosecutors and defense attorneys had sparred over how much of the boat jurors would get to see. The prosecution sought to remove a panel on which Tsarnaev allegedly scrawled incriminating messages so that jurors could see it with their own eyes. Defense attorney David Bruck argued that cutting out a panel would take the written words out of context and wouldn\\'t fairly reflect Tsarnaev\\'s state of mind. In South Boston, about a mile from the courthouse, jurors intently looked at the entire boat Monday. It had been loaded onto a semi truck and moved to the location for viewing by the jury. Dzhokhar Tsarnaev, wearing a dark jacket and no handcuffs or shackles, watched the jurors but showed little emotion or expression. Jurors appeared to strain to make out the words Tsarnaev scrawled inside the boat. The boat was riddled with more than 100 bullet holes -- some of which punctured Tsarnaev\\'s words. Aaron Cooper reported from Boston. Catherine E. Shoichet and Ralph Ellis wrote the story in Atlanta. CNN\\'s Jake Tapper and Ann O\\'Neill contributed to this report.', 'highlights': '3 police officers described an intense gun battle with Boston Marathon bomb suspects .\\nThey said the 2 men shot at them, threw bombs and tried to run them over .\\nJurors see the boat where Dzhokhar Tsarnaev hid before his arrest .', 'id': 'a63465aa8744a4a65c2c7050b301c21e4bf53e8a'}\n",
      "{'article': '(CNN)Pakistan\\'s highest court Friday ordered the release of Zaki-ur-Rehman Lakhvi, the alleged mastermind behind the Mumbai attacks, calling his detention illegal. Lakhvi, a top leader of the terrorist group Lashkar-e-Taiba, was not present at Friday\\'s court proceeding. The terror attacks in India left more than 160 people dead in November 2008. In the attacks, heavily armed men stormed landmark buildings around Mumbai, including luxury hotels, the city\\'s historic Victoria Terminus train station and a Jewish cultural center. On Friday, India summoned the Pakistan high commissioner \"to convey our strong feelings about (the) Lakhvi verdict,\" said India\\'s external affairs spokesman Syed Akbaruddin. Last year, the court granted Lakhvi bail, a decision the Pakistani government had said it would challenge. Many in India are still angry over the attacks and had criticized the bail decision. \"It is very disappointing that the accused of the Mumbai attacks has been granted bail,\" the nation\\'s home minister, Rajnath Singh, said in December. India executed the last surviving gunman from the attacks in 2012. Other suspects were all killed during the series of attacks, which went on for three days. CNN\\'s Harmeet Shah Singh contributed to this report.', 'highlights': 'The terror attacks in India left more than 160 people dead .\\nA  court granted the suspect bail last year .', 'id': 'e7203d308fc92da3852e54616a1cfe7bc3d2066c'}\n",
      "{'article': \"(CNN)Last November, the American people placed a great amount of trust in Republicans when they gave them complete control of Congress. Voters bought the illusion that a Republican Congress would govern effectively, help the middle class and focus on important issues like jobs and the economy. Now, after two months, that illusion has been shattered. Over the last two weeks, we have seen the Republican Congress manufacture, then escalate, a political crisis by threatening to shut down the Department of Homeland Security. There is no government task more basic than keeping citizens safe. And yet Republicans are recklessly putting our national security at risk to protect their own political security. By failing to do a full extension of DHS funding — holding it hostage with demands that the legislation roll back the President's immigration actions -- Republicans are leaving uncertain the livelihoods of more than 240,000 men and women who proudly serve as employees of the Border Patrol, Transportation Security Administration and other DHS agencies. Local law enforcement will continue to be denied access to the grants that help them keep our streets safe. The Federal Emergency Management Agency and other agencies that respond to natural disasters, like major winter storms, would have their operations disrupted. All so Republicans can try to score a few cheap political points with their base. But while this shutdown crisis is scary, what is even scarier is that this dysfunctional governing style is becoming a pattern with the Republican Party.  The Republican Congress seems more intent on bickering with itself, pushing an agenda to help special interests and catering to the most extreme wing of the party, than working for the middle class. These last two months have been a disgrace, a disservice to our country, and the American people won't soon forget what a reckless disaster Republicans in Congress have turned out to be.\", 'highlights': 'Ben Lujan: Americans voted in a GOP Congress on promise of effective governance that prioritized middle class .\\nHe says that was an illusion. On DHS funding, other issues, GOP has prioritized catering to its extreme wing and special interests .', 'id': '32d4bad025644097b0192d256a99e51f2d1ac585'}\n",
      "{'article': '(CNN)With the ongoing protests over the shooting death by police of black teenager Tony Robinson in Madison, Wisconsin, the racist chanting of fraternity members at the University of Oklahoma, and now the inexcusable shootings of two police officers in Ferguson, Missouri, it\\'s safe to say that the always-strained race relations in this country are being pushed to the breaking point. And the point with the most stress is the delicate relationship between police forces and the minority communities that they serve. To approach things from a more positive angle, the situation is dire enough that we absolutely have to try to uncover the good and not dwell on the negative. The best time to make things better is when it seems that everything is getting worse. This unacceptable status quo can motivate us to take the necessary steps to address the problems, which are not going to disappear unless we honestly deal with them. When things are going wrong, responsible people can begin by saying the right things. President Obama said on Twitter, \"Violence against police is unacceptable. Our prayers are with the officers in MO. Path to justice is one all of us must travel together.\" The Congressional Black Caucus issued a statement saying, \"The CBC understands the frustrations in Ferguson, but a response of violence is not the answer during this transformative moment in our country.\" And Attorney General Eric Holder noted, \"This was not someone trying to bring healing to Ferguson. ... This was a damn punk, a punk who was trying to sow discord.\" For the most part, authorities in Missouri have been careful not to blame the peaceful protesters. Surely, not all responses have been as measured, but the gravity of the situation will hopefully bring out the best in people. While we pause for a moment to let passions cool, we can use the time to consider how best to move forward with common resolve instead of mutual recrimination. The way forward is to engage citizens in the community -- to bring them into the room when decisions are being made about policing policies and procedures to make sure that those policies and procedures address the community\\'s real concerns. And it\\'s about putting law enforcement officers in the community as welcome members of that community -- as guarantors of the safety and security of the people instead of intimidating outside forces. This approach would benefit both the community and the police. I\\'m certain that officers would rather be appreciated and valued by the people they serve than be pressured to fill city coffers by issuing unnecessary citations, as noted in the DOJ report on Ferguson. Nobody becomes a cop because they secretly long to be a collection agent. One of the easiest ways to integrate law enforcement officers into the community is to physically put them on the sidewalks by increasing the number of cops who work good old-fashioned foot beats. Officers who view the world through a patrol car window are separated from the people they serve by more than a sheet of glass. Being encased in a vehicle alienates a person from the world around them. The cop on the beat is not just a quaint notion from old movies, he can be a bridge between police forces and the people they serve. Another idea is to give cops bicycles, which has brought so many law enforcement officials in touch with other cyclists in the community. Lack of community policing is one of the shortcomings cited in the DOJ report on Ferguson. In areas where the gulf between law enforcement and the neighborhood is too wide, mediators can be used to initially bring the two sides together. After all, both sides ultimately have the same goal of safe and peaceful neighborhoods. Both police departments and members of the community can take proactive steps to come together on more than a purely professional level.  A tech services company in the South Bronx recently hosted a video game competition with police officers and residents of the neighborhood. The event left local teenagers saying things about the cops like \"basically they\\'re like us.\" Ultimately, police should be considered members of the community -- a notion that needs to be encouraged by police departments and neighborhoods alike. Communities can make the local cops part of their neighborhood celebrations. New Orleans Police Det. Winston Harbin became a minor Internet celebrity for his impromptu dancing with local people during Mardi Gras. Besides just being fun, Harbin\\'s interaction with the community helped foster the type of mutual appreciation and respect that are essential to effective community policing. Fear and mistrust among minority communities toward police are the legacy of many decades of racism, unequal treatment, bias, subjective stereotyping and lack of opportunity. It is times like now, when that anger and resentment are boiling, that we address it. With the right approach, we can begin to change the attitude between the black community and the police from \"HandsUpDon\\'tShoot\" to \"HandsTogetherInTrust.\"', 'highlights': 'Latest Ferguson shootings push strained race relations to breaking point .\\nDonna Brazile: Relationship between police forces and minority communities must improve .', 'id': 'fee1bf4f253bb464e5efb38197d881adf7f4d82d'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_indices = random.sample(range(len(dataset)), 5)  # génère 5 indices uniques\n",
    "sampled_dataset = [dataset[i] for i in sample_indices]  # récupère les échantillons correspondants\n",
    "\n",
    "for article in sampled_dataset:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle baseline: TextRank\n",
    "\n",
    "TextRank est une méthode d’extraction de texte basée sur des algorithmes de classement de graphes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principe de TextRank**: des phrases dans un texte peuvent être représentées comme des nœuds dans un graphe, et les similitudes entre différentes phrases peuvent être représentées comme des arêtes reliant ces nœuds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des résumés avec TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "def summarize_textrank(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, 3)\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "baseline_summaries = [summarize_textrank(article['article']) for article in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du score Rouge\n",
    "\n",
    "Le score Rouge (Recall-Oriented Understudy for Gisting Evaluation) est une mesure standard pour évaluer la qualité des résumés générés par rapport à des résumés de référence.  \n",
    "Le score Rouge-1 mesure le chevauchement des unigrammes (mots individuels) entre le résumé généré et le résumé de référence. Le score Rouge-2 est l'équivalent pour les bigrammes (paires de mots). Le score Rouge-L est l'équivalent pour les plus longues séquences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROUGE moyen pour TextRank (baseline) : 0.20203824760177294\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "rouge_scores_baseline = [rouge.get_scores(baseline_summary, article['highlights']) for baseline_summary, article in zip(baseline_summaries, dataset)]\n",
    "\n",
    "print(\"Score ROUGE moyen pour TextRank (baseline) :\", np.mean([score[0]['rouge-1']['f'] for score in rouge_scores_baseline]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle T5  \n",
    "\n",
    "Text-to-Text Transfer Transformer : architecture de modèle NLP développée par Google Research.  \n",
    "L’architecture suit la structure encodeur / décodeur.  \n",
    "L'encodeur lit et encode le texte d'entrée en vecteurs de caractéristiques.  \n",
    "Le décodeur utilise les vecteurs de caractéristiques pour générer le texte de sortie, mot par mot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données\n",
    "\n",
    "Division du dataset en ensembles d'entraînement, de validation et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset d'entraînement : 107\n",
      "Taille du dataset de validation : 13\n",
      "Taille du dataset de test : 14\n"
     ]
    }
   ],
   "source": [
    "# Définition des proportions pour le split\n",
    "train_size = 0.8\n",
    "val_size = 0.1  # et test_size sera de 0.1 également\n",
    "\n",
    "# Calcul des indices de coupe pour le dataset\n",
    "train_split = int(len(dataset) * train_size)\n",
    "val_split = train_split + int(len(dataset) * val_size)\n",
    "\n",
    "train_dataset = dataset.select(range(0, train_split))\n",
    "val_dataset = dataset.select(range(train_split, val_split))\n",
    "test_dataset = dataset.select(range(val_split, len(dataset)))\n",
    "\n",
    "# Taille de chaque split\n",
    "print(\"Taille du dataset d'entraînement :\", len(train_dataset))\n",
    "print(\"Taille du dataset de validation :\", len(val_dataset))\n",
    "print(\"Taille du dataset de test :\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement des données\n",
    "\n",
    "On prétraite les données pour le formatage requis par T5. Cela inclut l'ajout du préfixe \"summarize:\" devant chaque texte et la conversion des données au format approprié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    # prétraitement de chaque article et highlight dans le lot\n",
    "    input_texts = ['summarize: ' + article for article in batch['article']]\n",
    "    input_encodings = tokenizer(input_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    label_encodings = tokenizer(batch['highlights'], padding='max_length', truncation=True, max_length=150, return_tensors='pt')\n",
    "\n",
    "    # on s'assure que 'input_ids' et 'labels' sont des listes de ids\n",
    "    batch['input_ids'] = input_encodings.input_ids\n",
    "    batch['labels'] = label_encodings.input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning du modèle\n",
    "\n",
    "Configuration et fine-tuning du modèle T5 sur l'ensemble de données d'entraînement.\n",
    "\n",
    "Des techniques d'optimisation sont employées pour améliorer la convergence du modèle.\n",
    "\n",
    "1. **Warmup Linéaire** : L'argument `warmup_steps=500` dans `TrainingArguments` indique que le warmup linéaire est utilisé. Pendant les 500 premières étapes d'entraînement, le taux d'apprentissage augmente linéairement depuis zéro jusqu'au taux d'apprentissage initial défini (non spécifié explicitement, donc il utilise la valeur par défaut du `Trainer`). Cette technique aide à stabiliser l'entraînement au début, en évitant les mises à jour trop importantes qui pourraient nuire à la convergence initiale du modèle.\n",
    "\n",
    "2. **Décroissance du Poids (Weight Decay)** : L'argument `weight_decay=0.01` signifie qu'une pénalité est ajoutée à la perte, proportionnelle aux poids du modèle, encourageant ainsi le modèle à apprendre des poids plus petits (en valeur absolue). Cette technique est utilisée pour régulariser le modèle et prévenir le surajustement, ce qui peut conduire à une meilleure généralisation sur des données non vues.\n",
    "\n",
    "Le warmup linéaire assure que l'entraînement démarre doucement, réduisant le risque de perturbations importantes dues à des mises à jour trop agressives au début de l'entraînement. La décroissance du poids encourage le modèle à rester simple, ce qui peut améliorer la capacité du modèle à généraliser à partir de l'ensemble d'entraînement à l'ensemble de validation ou de test, améliorant ainsi la performance globale du modèle sur des données nouvelles ou non vues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération de résumés et calcul des scores ROUGE\n",
    "\n",
    "Après le fine-tuning, on utilise le modèle pour générer des résumés sur l'ensemble de test et calculer les scores ROUGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.38149605386052954, 'p': 0.30319058746522215, 'f': 0.32774647502072046}, 'rouge-2': {'r': 0.17753578028822428, 'p': 0.13766313299248464, 'f': 0.15022297710984672}, 'rouge-l': {'r': 0.3415781449720323, 'p': 0.2701278099171505, 'f': 0.29195519074747683}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "generated_summaries = []\n",
    "references = []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    input_ids = tokenizer('summarize: ' + batch['article'], return_tensors='pt', truncation=True, padding='max_length', max_length=512).input_ids\n",
    "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    generated_summaries.append(generated_summary)\n",
    "    references.append(batch['highlights'])\n",
    "\n",
    "rouge_scores = rouge.get_scores(generated_summaries, references, avg=True)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROUGE moyen pour T5 avec entraînement : 0.32774647502072046\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# scores ROUGE pour chaque paire de résumé généré et de référence\n",
    "rouge_scores_finetuned = [rouge.get_scores(gen_summary, ref) for gen_summary, ref in zip(generated_summaries, references)]\n",
    "\n",
    "# moyenne des scores F1 de ROUGE-1 pour tous les résumés\n",
    "average_rouge1_f_score_finetuned = np.mean([score[0]['rouge-1']['f'] for score in rouge_scores_finetuned])\n",
    "print(\"Score ROUGE moyen pour T5 avec entraînement :\", average_rouge1_f_score_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation du modèle: changement du nombre d'epochs\n",
    "\n",
    "On modifie le nombre d'epochs pour voir si cela peut augmenter les performances du modèle T5 en terme de score Rouge.  \n",
    "Pour ce faire on crée un nouveau modèle t5-small avec un num_train_epochs égal à 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 10/108 [00:41<06:53,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6083, 'grad_norm': 50.46287536621094, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 20/108 [01:23<06:07,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5346, 'grad_norm': 38.754150390625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 27/108 [01:54<06:13,  4.62s/it]\n",
      " 25%|██▌       | 27/108 [01:59<06:13,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.298940658569336, 'eval_runtime': 4.8226, 'eval_samples_per_second': 2.696, 'eval_steps_per_second': 0.829, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 30/108 [02:14<07:09,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3132, 'grad_norm': 42.01673889160156, 'learning_rate': 3e-06, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 40/108 [03:03<05:14,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.9847, 'grad_norm': 57.50969314575195, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 50/108 [03:46<04:16,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6654, 'grad_norm': 39.15045166015625, 'learning_rate': 5e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 54/108 [04:04<03:46,  4.20s/it]\n",
      " 50%|█████     | 54/108 [04:07<03:46,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.37966251373291, 'eval_runtime': 3.5917, 'eval_samples_per_second': 3.619, 'eval_steps_per_second': 1.114, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 60/108 [04:35<03:45,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9689, 'grad_norm': 62.37910461425781, 'learning_rate': 6e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 70/108 [05:18<02:47,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.3657, 'grad_norm': 64.84869384765625, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 80/108 [06:02<02:02,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.1037, 'grad_norm': 47.683414459228516, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 81/108 [06:06<01:53,  4.20s/it]\n",
      " 75%|███████▌  | 81/108 [06:10<01:53,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.969048023223877, 'eval_runtime': 3.7733, 'eval_samples_per_second': 3.445, 'eval_steps_per_second': 1.06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 90/108 [06:49<01:16,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6568, 'grad_norm': 25.416622161865234, 'learning_rate': 9e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 100/108 [07:29<00:32,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0439, 'grad_norm': 37.55124282836914, 'learning_rate': 1e-05, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [08:03<00:00,  3.94s/it]\n",
      "100%|██████████| 108/108 [08:06<00:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.074146747589111, 'eval_runtime': 3.458, 'eval_samples_per_second': 3.759, 'eval_steps_per_second': 1.157, 'epoch': 4.0}\n",
      "{'train_runtime': 486.7038, 'train_samples_per_second': 0.879, 'train_steps_per_second': 0.222, 'train_loss': 8.060311705977828, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=108, training_loss=8.060311705977828, metrics={'train_runtime': 486.7038, 'train_samples_per_second': 0.879, 'train_steps_per_second': 0.222, 'train_loss': 8.060311705977828, 'epoch': 4.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_epochs = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_epochs,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.4126912869174646, 'p': 0.30067283870248923, 'f': 0.3396914456172121}, 'rouge-2': {'r': 0.20694422178202626, 'p': 0.14228825302140938, 'f': 0.16532284488394994}, 'rouge-l': {'r': 0.38670389195948146, 'p': 0.2788579161522726, 'f': 0.31633109813373717}}\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "generated_epochs_summaries = []\n",
    "references = []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    input_ids = tokenizer('summarize: ' + batch['article'], return_tensors='pt', truncation=True, padding='max_length', max_length=512).input_ids\n",
    "    summary_ids = model_epochs.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    generated_epochs_summaries.append(generated_summary)\n",
    "    references.append(batch['highlights'])\n",
    "\n",
    "rouge_scores = rouge.get_scores(generated_epochs_summaries, references, avg=True)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score ROUGE moyen pour T5 avec entraînement : 0.33969144561721215\n"
     ]
    }
   ],
   "source": [
    "rouge_scores_finetuned = [rouge.get_scores(gen_summary, ref) for gen_summary, ref in zip(generated_epochs_summaries, references)]\n",
    "\n",
    "average_rouge1_f_score_finetuned = np.mean([score[0]['rouge-1']['f'] for score in rouge_scores_finetuned])\n",
    "print(\"Score ROUGE moyen pour T5 avec entraînement :\", average_rouge1_f_score_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En passant le nombre d'epochs de 3 à 4, le score Rouge a légèrement augmenté. Il est passé de 0,327 à 0,339."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 14\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des articles et résumés générés dans un fichier CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "articles = [item['article'] for item in test_dataset]\n",
    "highlights = [item['highlights'] for item in test_dataset]\n",
    "\n",
    "# création d'un DataFrame avec les articles et les résumés générés\n",
    "df = pd.DataFrame({\n",
    "    'article': articles,\n",
    "    'highlights': highlights,\n",
    "    't5_summary': generated_epochs_summaries\n",
    "})\n",
    "\n",
    "# sauvegarde dans un fichier CSV\n",
    "df.to_csv('cnn_daily_t5.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p7env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
